import json
from pathlib import Path
import os
import gc
import time
from datetime import datetime, timedelta
import torch
from typing import List
import multiprocessing
from tqdm import tqdm
import shutil

# LangChainã¨é–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from optimum.onnxruntime import ORTModelForFeatureExtraction
from transformers import AutoTokenizer
from langchain_chroma import Chroma

# --- è¨­å®šé …ç›® ---
CLEANED_DATA_PATH = Path("outputs/cleaned_toshin_data.json")
VECTOR_STORE_DIR = Path("vector_store")

# ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ä¸Šæ›¸ãã•ã‚Œã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼‰
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-small"

# ãƒãƒ£ãƒ³ã‚¯è¨­å®š
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

# --- ONNXé«˜é€ŸåŒ–ã®ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹ ---
class ONNXEmbeddings:
    """ONNXãƒ¢ãƒ‡ãƒ«ã‚’LangChainã§ä½¿ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    def __init__(self, model, tokenizer, device="cpu"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
        encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}
        with torch.no_grad():
            model_output = self.model(**encoded_input)
        
        sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
        return sentence_embeddings.cpu().tolist()

    def embed_query(self, text: str) -> List[float]:
        return self.embed_documents([text])[0]

# --- ä¸¦åˆ—å‡¦ç†ã®ãŸã‚ã®ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼ˆé€²æ—å¯è¦–åŒ–å¯¾å¿œï¼‰ ---
def embed_batch_worker_with_progress(args):
    """
    ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚Œã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ã€‚
    å€‹åˆ¥ã®é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
    """
    worker_id, texts = args  # å¼•æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯
    
    try:
        model = ORTModelForFeatureExtraction.from_pretrained(
            EMBEDDING_MODEL_NAME,
            file_name="model_qint8_avx512_vnni.onnx"
        )
        tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
        embeddings_model = ONNXEmbeddings(model=model, tokenizer=tokenizer)
        
        pbar = tqdm(total=len(texts),
                    desc=f"  Worker-{worker_id:02d}",
                    position=worker_id + 1,
                    leave=False)
        
        results = []
        inner_batch_size = 64
        for i in range(0, len(texts), inner_batch_size):
            batch = texts[i:i + inner_batch_size]
            results.extend(embeddings_model.embed_documents(batch))
            pbar.update(len(batch))
            
        pbar.close()
        return results
    except Exception as e:
        if 'pbar' in locals() and pbar:
            pbar.close()
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ—ãƒ­ã‚»ã‚¹IDã‚’æ˜ç¢ºã«è¡¨ç¤º
        print(f"\nâŒ Error in Worker-{worker_id}: {e}")
        # ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return []


# --- ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†é–¢æ•° ---
def build_vector_store():
    """ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ§‹ç¯‰ï¼ˆé€²æ—å¯è¦–åŒ–ãƒ»æœ€çµ‚ç¢ºå®šç‰ˆï¼‰"""
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if not CLEANED_DATA_PATH.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {CLEANED_DATA_PATH}")
        return
    print(f"ğŸ“‚ '{CLEANED_DATA_PATH}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    with open(CLEANED_DATA_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ãƒ†ã‚¹ãƒˆç”¨
    # data = data[:3]
    
    total_docs = len(data)
    print(f"  â¡ï¸ {total_docs:,}ä»¶ã®æ–‡æ›¸ã‚’æ¤œå‡º")

    # 2. ONNXãƒ¢ãƒ‡ãƒ«ã®äº‹å‰æº–å‚™
    print("\n  ğŸ” ONNXãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç¢ºèªãƒ»æº–å‚™ã—ã¦ã„ã¾ã™...")
    try:
        model = ORTModelForFeatureExtraction.from_pretrained(
            EMBEDDING_MODEL_NAME,
            file_name="model_qint8_avx512_vnni.onnx"
        )
        tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
    except Exception:
        print("    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ONNXãƒ¢ãƒ‡ãƒ«ã®å¤‰æ›ã‚’ä¸€åº¦ã ã‘è¡Œã„ã¾ã™...")
        model = ORTModelForFeatureExtraction.from_pretrained(EMBEDDING_MODEL_NAME, export=True)
        tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)
        try:
            model_id = EMBEDDING_MODEL_NAME.replace('/', '--')
            cache_dir = Path.home() / ".cache/huggingface/hub" / f"models--{model_id}"
            snapshot_path = next(cache_dir.glob("snapshots/*"))
            onnx_dir = snapshot_path / "onnx"
            if (onnx_dir / "model.onnx").exists(): (onnx_dir / "model.onnx").unlink()
            if (onnx_dir / "model_O4.onnx").exists(): (onnx_dir / "model_O4.onnx").unlink()
            print("    âœ… ä¸è¦ãªONNXãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"    âš ï¸ è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«è»½å¾®ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    embeddings_function = ONNXEmbeddings(model=model, tokenizer=tokenizer)
    print("  âœ… ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åˆæœŸåŒ–
    print(f"\nğŸ’¾ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™ @ '{VECTOR_STORE_DIR}'...")
    if VECTOR_STORE_DIR.exists():
        response = input("  æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ä¸Šæ›¸ãã—ã¾ã™ã‹ï¼Ÿ (y/n): ")
        if response.lower() == 'y':
            shutil.rmtree(VECTOR_STORE_DIR)
            print("  âœ… æ—¢å­˜ã®ã‚¹ãƒˆã‚¢ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
    vector_store = Chroma(
        persist_directory=str(VECTOR_STORE_DIR),
        embedding_function=embeddings_function
    )

    # 4. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®æº–å‚™
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
    )

    # 5. ä¸¦åˆ—ãƒ—ãƒ­ã‚»ã‚¹æ•°ã®æ±ºå®š
    print(f"\nğŸ”„ ä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    if "base" in EMBEDDING_MODEL_NAME or "large" in EMBEDDING_MODEL_NAME:
        num_processes = 4
        print(f"  ğŸ§  base/largeãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã€ãƒ¡ãƒ¢ãƒªã‚’è€ƒæ…®ã—ã¦ãƒ—ãƒ­ã‚»ã‚¹æ•°ã‚’ {num_processes} ã«åˆ¶é™ã—ã¾ã™ã€‚")
    else:
        num_processes = max(1, os.cpu_count() - 2)
        print(f"  âš¡ smallãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã€CPUæ€§èƒ½ã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹æ•° {num_processes} ã‚’è¨­å®šã—ã¾ã™ã€‚")
    print(f"  âš™ï¸ è¨­å®š: ãƒ¢ãƒ‡ãƒ«={EMBEDDING_MODEL_NAME.split('/')[-1]}, ä¸¦åˆ—ãƒ—ãƒ­ã‚»ã‚¹æ•°={num_processes}")
    
    start_time = time.time()
    
    # 6. å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸€æ‹¬ä½œæˆ
    print("\n  ğŸ“¦ å…¨æ–‡æ›¸ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆä¸­...")
    all_docs_to_process = []
    for record in tqdm(data, desc="    Creating all chunks"):
        base_metadata = {
            "source_url": record.get("URL", ""), "case_name": record.get("äº‹ä»¶å", ""),
            "agency": record.get("è«®å•åº", ""), "report_date": record.get("ç­”ç”³æ—¥_iso", "")
        }
        all_texts = []
        summary = record.get("summary_text", "")
        if summary and summary.strip(): all_texts.append(("summary", summary))
        for section, text in record.get("detail_texts", {}).items():
            if text and text.strip(): all_texts.append((section, text))
        
        for text_type, text in all_texts:
            chunks = text_splitter.split_text(text)
            for i, chunk_content in enumerate(chunks):
                metadata = base_metadata.copy()
                metadata["type"] = text_type
                metadata["chunk_index"] = i
                all_docs_to_process.append(Document(page_content=chunk_content, metadata=metadata))

    print(f"  â¡ï¸ ä½œæˆã•ã‚ŒãŸç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(all_docs_to_process):,}")

    # 7. ä¸¦åˆ—å‡¦ç†ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    print(f"\n  âš™ï¸  {num_processes}å€‹ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ä¸¦åˆ—ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œä¸­...")
    texts_to_embed = [doc.page_content for doc in all_docs_to_process]
    
    num_texts = len(texts_to_embed)
    # å®Ÿéš›ã«èµ·å‹•ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹æ•°ã‚’ã€ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹æ•°ã«é™å®šã™ã‚‹
    actual_num_processes = min(num_processes, num_texts)
    
    tasks = []
    if actual_num_processes > 0:
        chunk_size_per_process = (num_texts + actual_num_processes - 1) // actual_num_processes
        for i in range(actual_num_processes):
            start = i * chunk_size_per_process
            end = min((i + 1) * chunk_size_per_process, num_texts)
            tasks.append((i, texts_to_embed[start:end]))

    embeddings = []
    if tasks: # ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã®ã¿ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ
        with tqdm(total=num_texts, desc="Overall Progress     ", position=0) as overall_pbar:
            with multiprocessing.Pool(processes=actual_num_processes) as pool:
                for result_batch in pool.imap_unordered(embed_batch_worker_with_progress, tasks):
                    embeddings.extend(result_batch)
                    overall_pbar.update(len(result_batch))
    
    print("\n") # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢ã«ã™ã‚‹ãŸã‚ã®æ”¹è¡Œ

# 8. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒãƒƒãƒã§åˆ†å‰²ã—ã¦è¿½åŠ 
    print("\n\n  ğŸ’¾ è¨ˆç®—æ¸ˆã¿ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åˆ†å‰²ã—ã¦è¿½åŠ ä¸­...")
    if texts_to_embed:
        db_batch_size = 2048  # ä¸€åº¦ã«DBã¸æ›¸ãè¾¼ã‚€ãƒãƒ£ãƒ³ã‚¯æ•°
        
        for i in tqdm(range(0, len(texts_to_embed), db_batch_size), desc="    Adding to DB"):
            start_index = i
            end_index = min(i + db_batch_size, len(texts_to_embed))
            
            vector_store.add_texts(
                texts=texts_to_embed[start_index:end_index],
                metadatas=[doc.metadata for doc in all_docs_to_process[start_index:end_index]],
                embeddings=[e for e in embeddings[start_index:end_index]]
            )
        print("  âœ… å®Œäº†ï¼ï¼ˆãƒ‡ãƒ¼ã‚¿ã¯è‡ªå‹•çš„ã«æ°¸ç¶šåŒ–ã•ã‚Œã¾ã™ï¼‰")
    else:
        print("  â„¹ï¸ è¿½åŠ ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        
    # 9. å®Œäº†å ±å‘Š
    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"ğŸ‰ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"{'='*60}")
    print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"  - å‡¦ç†ã—ãŸæ–‡æ›¸æ•°: {total_docs:,}")
    print(f"  - ä½œæˆã—ãŸãƒãƒ£ãƒ³ã‚¯æ•°: {len(all_docs_to_process):,}")
    print(f"  - ç·å‡¦ç†æ™‚é–“: {str(timedelta(seconds=int(total_time)))}")
    if total_time > 0 and len(all_docs_to_process) > 0:
        avg_speed = len(all_docs_to_process) / total_time
        print(f"  - å¹³å‡é€Ÿåº¦: {avg_speed:.1f} chunks/sec")
    print(f"  - ä¿å­˜å…ˆ: {VECTOR_STORE_DIR}")


if __name__ == '__main__':
    multiprocessing.freeze_support()
    import sys
    
    model_map = {
        "small": "intfloat/multilingual-e5-small",
        "base": "intfloat/multilingual-e5-base",
        "large": "intfloat/multilingual-e5-large"
    }
    
    selected_model = EMBEDDING_MODEL_NAME
    
    if len(sys.argv) > 1:
        model_arg = sys.argv[1]
        if model_arg in model_map:
            selected_model = model_map[model_arg]
            print(f"ğŸ¯ {model_arg}ãƒ¢ãƒ‡ãƒ«ã‚’å¼•æ•°ã¨ã—ã¦é¸æŠã—ã¾ã—ãŸã€‚")
        else:
            print(f"âš ï¸ æœªçŸ¥ã®å¼•æ•° '{model_arg}' ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            print(f"  ä½¿ç”¨å¯èƒ½ãªå¼•æ•°: {', '.join(model_map.keys())}")
    
    EMBEDDING_MODEL_NAME = selected_model
    print(f"    ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {EMBEDDING_MODEL_NAME}")
    
    build_vector_store()